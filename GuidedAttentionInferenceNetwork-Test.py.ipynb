{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this paper: http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Tell_Me_Where_CVPR_2018_paper.pdf\n",
    "Try to come up with an implementation of the GAIN network. (For now just the GAIN, not GAIN_ext)\n",
    "\n",
    "While I'm not 100% sure what this consists of, I beielive it is as follows:\n",
    " 1. A CNN\n",
    " 2. into a fully connected Layer\n",
    " 3. which outputs a classification loss\n",
    " 4. which is fed into another fully connected layer\n",
    "    (which is stacked together into a ReLu function??? I'm not sure what this step is in the diagram.)\n",
    " 5. which outputs an Attention Map\n",
    " 6. which is used to apply a soft mask onto the original image\n",
    " 7. which is fed into another CNN\n",
    " 8. into another fully connected layer\n",
    " 9. which outputs an Attention Mining Loss\n",
    "10. The Attention Mining Loss is then combined which the Classification loss\n",
    "    (which is then used for backprop???)\n",
    "    \n",
    "    \n",
    "Notes on implementation (from the paper)\n",
    "- It says that in the Classifcation Stream (steps 1-4 above) that that gradients dlowing back should pass through a global average pooling layer, to obtain the weights. I'm not sure how this works exactly, but in the paper it shows a function that the \n",
    "    - weight \n",
    "        - for class c\n",
    "        - of unit k\n",
    "        - in the l-th layer\n",
    "    - should be calulated by taking the GAP (global average pool) of the partial derivative of\n",
    "        - score of class c, DIVIDED BY\n",
    "        - activation (f) of unit k in the l-th layer (which is just the previous weight, no?)\n",
    "- Apparently the above is NOT used for back prop. Rather the weight is the \"importance\" of the actiavtion map (fkl) supporting class c.\n",
    "    - its says that the weights matrix obtained for all layers and units is used as a kernel to apply a 2D convolution for the activation map matrix fl, to integrate all activation maps. Which is then followed by a ReLU function (so i'm assuming this is step 4 above), which should output an attention map.\n",
    "    - Well if I can get that first CNN to ouput that attention map, that would be a great first start.\n",
    "    - Ac = ReLu(conv(fl, wc))\n",
    "    \n",
    "This is a good github repo of an implementation in PyTorch:\n",
    "https://github.com/AustinDoolittle/Pytorch-Gain/blob/master/gain.py\n",
    "- I kindof like keras better, so lets try keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Reading:\n",
    "- Attention Maps\n",
    "    - https://jacobgil.github.io/deeplearning/class-activation-maps\n",
    "    - https://github.com/jacobgil/keras-cam\n",
    "    - https://github.com/tdeboissiere/VGG16CAM-keras\n",
    "\n",
    "- Getting Gradients\n",
    "    - https://www.programcreek.com/python/example/93762/keras.backend.gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is just copied over from the baseline_cnn_notebook\n",
    "# (with some modification)\n",
    "\n",
    "class ModelParameters:\n",
    "\n",
    "    def __init__(self,\n",
    "                 training_data_path,\n",
    "                 num_classes=28,\n",
    "                 num_epochs=5,\n",
    "                 batch_size=16,\n",
    "                 image_rows=1708,\n",
    "                 image_cols=1708,\n",
    "                 row_scale_factor=4,\n",
    "                 col_scale_factor=4,\n",
    "                 n_channels=3,\n",
    "                 using_imagenet=False,\n",
    "                 shuffle=False):\n",
    "        \n",
    "        self.training_data_path = training_data_path\n",
    "        self.num_classes = num_classes\n",
    "        # what does n_epochs mean? it seems we pass this into the \"epochs\"\n",
    "        # parameter on the \"fit_generator\" method on our keras model\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.row_dimension = np.int(image_rows / row_scale_factor)\n",
    "        self.col_dimension = np.int(image_cols / col_scale_factor)\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        if using_imagenet:\n",
    "            self.row_dimension = 224\n",
    "            self.col_dimension = 224\n",
    "            self.n_channels = 3\n",
    "\n",
    "\n",
    "class ImagePreprocessor:\n",
    "\n",
    "    def __init__(self, modelParameters):\n",
    "        self.image_path = modelParameters.training_data_path\n",
    "        self.n_channels = modelParameters.n_channels\n",
    "        self.row_dimension = modelParameters.row_dimension\n",
    "        self.col_dimension = modelParameters.col_dimension\n",
    "\n",
    "    def preprocess(self, image):\n",
    "        image = cv2.resize(image, (self.row_dimension, self.col_dimension))\n",
    "        # image = np.reshape(image, (image.shape[0], image.shape[1], self.n_channels))\n",
    "        image /= 255\n",
    "        return image\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        image = cv2.imread(self.image_path + image_id + '.jpg')\n",
    "        if self.n_channels == 1:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # Convert Color to Greyscalle (1 channel)\n",
    "            np_image = np.zeros(shape=(image.shape[0], image.shape[1]))\n",
    "        else:\n",
    "            np_image = np.zeros(shape=(image.shape[0], image.shape[1], self.n_channels))\n",
    "        np_image[:,:] = image\n",
    "        return np_image\n",
    "\n",
    "class ImageBatchGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, image_ids, dataframe, model_params, image_processor):\n",
    "        '''\n",
    "        Writing a child implementation of keras.utils.Sequence will help us\n",
    "        manage our batches of data.\n",
    "        Each sequence must implement __len__ and __getitem__\n",
    "        This structure guarantees that the network will only train once on each\n",
    "        sample per epoch which is not the case with generators.\n",
    "        \n",
    "        We can use this class to instantiate training and validation generators\n",
    "        that we can pass into our keras model like:\n",
    "        \n",
    "        training_generator = ImageBatchLoader(...)\n",
    "        validation_generator = ImageBatchLoader(...)\n",
    "        model.set_generators(training_generator, validation_generator)\n",
    "        '''\n",
    "        self.image_ids = image_ids\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "        # Helper classes\n",
    "        self._imageProcessor = image_processor\n",
    "\n",
    "        # Training parameters\n",
    "        self.batch_size = model_params.batch_size\n",
    "        self.dimensions = (model_params.row_dimension, model_params.col_dimension)\n",
    "        self.n_channels = model_params.n_channels\n",
    "        self.shuffle = model_params.shuffle\n",
    "\n",
    "        # Run on_epoch_end in _init_ to init our first image batch\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        '''\n",
    "        Tensorflow will run this method at the end of each epoch\n",
    "        So this is where we will modify our batch.\n",
    "        '''\n",
    "        self.indexes = np.arange(len(self.image_ids))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __len__(self):\n",
    "        # Denotes the number of batchs per epoch\n",
    "        return int(np.floor(len(self.image_ids) / self.batch_size))\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # Get this batches indexes\n",
    "        indexes = self.indexes[index * self.batch_size:(index+1) * self.batch_size]\n",
    "\n",
    "        # Get cooresponding image Ids\n",
    "        batch_image_ids = [self.image_ids[i] for i in indexes]\n",
    "\n",
    "        # Generate one batch of data\n",
    "        X, y = self.__generator(batch_image_ids)\n",
    "        return X, y\n",
    "    \n",
    "    def __generator(self, batch_image_ids):\n",
    "\n",
    "        def get_target_class(imageid):\n",
    "            # .loc will lookup the row where the passed in statement is true\n",
    "            target = self.dataframe.loc[self.dataframe.imageId == imageid]\n",
    "            target = target.lesion.values[0]\n",
    "            return target\n",
    "\n",
    "        X = np.empty((self.batch_size, *self.dimensions, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        for index, imageid in enumerate(batch_image_ids):\n",
    "            image = self._imageProcessor.load_image(imageid)\n",
    "            image = self._imageProcessor.preprocess(image)\n",
    "\n",
    "            X[index] = image\n",
    "            y[index] = get_target_class(imageid)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "class PredictGenerator:\n",
    "\n",
    "    def __init__(self, image_ids, image_processor, image_path):\n",
    "        self.image_processor = image_processor\n",
    "        self.image_processor.image_path = image_path\n",
    "        self.image_ids = image_ids\n",
    "\n",
    "    def predict(self, model):\n",
    "        y = np.empty(shape=(len(self.image_ids)))\n",
    "        for n in range(len(self.image_ids)):\n",
    "            image = self.image_processor.load_image(self.image_ids[n])\n",
    "            image = self.image_processor.preprocess(image)\n",
    "            image = image.reshape((1, *image.shape))\n",
    "            y[n] = model.predict(image)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import load_model, Model\n",
    "from keras.applications import DenseNet121\n",
    "\n",
    "class GAIN_Model:\n",
    "    \n",
    "    def __init__(self, model_params):\n",
    "        self.img_rows = model_params.row_dimension\n",
    "        self.img_cols = model_params.col_dimension\n",
    "        self.n_channels = model_params.n_channels\n",
    "        self.input_shape = (self.img_rows, self.img_cols, self.n_channels)\n",
    "        self.num_epochs = model_params.num_epochs\n",
    "        # What metrics should I use for generating the attention map?\n",
    "        # The above linked PyTorch implemntation just uses Accuracy, so I'll stick with that for now.\n",
    "        self.metrics = ['accuracy']\n",
    "        \n",
    "    def build_model(self):\n",
    "        \n",
    "#         def global_average_pooling(x):\n",
    "#             return K.mean(x, axis = (2,3))\n",
    "\n",
    "#         def global_average_pool_shape(input_shape):\n",
    "#             return input_shape[0:2]\n",
    "\n",
    "#         model.add(Lambda(\n",
    "#             global_average_pooling, \n",
    "#             output_shape = global_average_pooling_shape)) # Note to self, look up what Lambda does\n",
    "        \n",
    "        base_model = DenseNet121(\n",
    "            include_top=False, \n",
    "            weights='imagenet')\n",
    "        \n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(1024,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
    "        x = Dense(1024,activation='relu')(x) #dense layer 2\n",
    "        x = Dense(512,activation='relu')(x) #dense layer 3\n",
    "        preds = Dense(1,activation='sigmoid')(x) #final layer with softmax activation\n",
    "        # model.add(Dense(1, activation='softmax', init='uniform'))\n",
    "        \n",
    "        self.model = Model(inputs=base_model.input, outputs=preds)\n",
    "        \n",
    "    def build_model(self):\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Interesting, it looks like in that above linked PyTorch implementation, the classification model\n",
    "        # is just passed in as an argument.\n",
    "        # Using the keras-cam repo from jacobgil as inspiration, lets output an attention map\n",
    "        model = self.VGG16_convolutions()\n",
    "        # Load weights here\n",
    "        \n",
    "        \n",
    "        # In the keras-cam repo the Dense layer is set to 2.\n",
    "        # I'm setting it to 1 because I have only 1 class, not sure if this is right, but I'll find out soon enough\n",
    "        \n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def compile_model(model):\n",
    "        sgd = SGD(lr=0.01, decay=1e-6, momentum=0.5, nesterov=True)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=self.metrics)\n",
    "        return model\n",
    "    \n",
    "    def set_generators(self, train_generator, validation_generator):\n",
    "        self.training_generator = train_generator\n",
    "        self.validation_generator = validation_generator\n",
    "        \n",
    "    def learn(self, model):\n",
    "        return model.fit_generator(\n",
    "            generator=self.training_generator,\n",
    "            validation_data=self.validation_generator,\n",
    "            epochs=self.num_epochs,\n",
    "            use_multiprocessing=True,\n",
    "            workers=8,\n",
    "            verbose=1)\n",
    "\n",
    "    def score(self, model):\n",
    "        return model.evaluate_generator(\n",
    "            generator=self.validation_generator,\n",
    "            use_multiprocessing=True,\n",
    "            workers=8)\n",
    "\n",
    "    def predict(self, predict_generator, model):\n",
    "        y = predict_generator.predict(model)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"./lesion-csv.csv\")\n",
    "train_df = pd.read_csv(\"./train.csv\")\n",
    "test_df = pd.read_csv(\"./test.csv\")\n",
    "\n",
    "# All we care about at this point is the id and class\n",
    "train_df = train_df.drop(\n",
    "    [\"Unnamed: 0\", \"Unnamed: 0.1\", \"teethNumbers\", \"description\", \"numberOfCanals\", \"date\", \"sequenceNumber\"], axis=1)\n",
    "\n",
    "test_df = test_df.drop(\n",
    "    [\"Unnamed: 0\", \"Unnamed: 0.1\", \"teethNumbers\", \"description\", \"numberOfCanals\", \"date\", \"sequenceNumber\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = {\n",
    "    \"train\": train_df.imageId.values,\n",
    "    \"validation\": test_df.imageId.values,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './lesion_images/all_images_processed/'\n",
    "\n",
    "model_params = ModelParameters(\n",
    "    train_path,\n",
    "    num_epochs=10,\n",
    "    batch_size=16,\n",
    "    image_rows=224,\n",
    "    image_cols=224,\n",
    "    row_scale_factor=1,\n",
    "    col_scale_factor=1)\n",
    "\n",
    "image_processor = ImagePreprocessor(model_params)\n",
    "\n",
    "\n",
    "training_generator = ImageBatchGenerator(partition['train'], data, model_params, image_processor)\n",
    "validation_generator = ImageBatchGenerator(partition['validation'], data, model_params, image_processor)\n",
    "predict_generator = PredictGenerator(partition['validation'], image_processor, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The channel dimension of the inputs should be defined. Found `None`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b7d5d55d991c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mModelHelper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAIN_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelHelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelHelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mModelHelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_generators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelHelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-0a521e24c9e5>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# is just passed in as an argument.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# Using the keras-cam repo from jacobgil as inspiration, lets output an attention map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVGG16_convolutions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;31m# Load weights here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         model.add(Lambda(\n",
      "\u001b[0;32m<ipython-input-32-0a521e24c9e5>\u001b[0m in \u001b[0;36mVGG16_convolutions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZeroPadding2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZeroPadding2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_source_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                                          \u001b[0;34m'You can build it manually via: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[0;32m--> 431\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mchannel_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchannel_axis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             raise ValueError('The channel dimension of the inputs '\n\u001b[0m\u001b[1;32m    133\u001b[0m                              'should be defined. Found `None`.')\n\u001b[1;32m    134\u001b[0m         \u001b[0minput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchannel_axis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The channel dimension of the inputs should be defined. Found `None`."
     ]
    }
   ],
   "source": [
    "ModelHelper = GAIN_Model(model_params)\n",
    "model = ModelHelper.build_model()\n",
    "model = ModelHelper.compile_model(model)\n",
    "ModelHelper.set_generators(training_generator, validation_generator)\n",
    "model = ModelHelper.learn(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
