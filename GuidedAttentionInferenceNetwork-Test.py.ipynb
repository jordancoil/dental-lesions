{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this paper: http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Tell_Me_Where_CVPR_2018_paper.pdf\n",
    "Try to come up with an implementation of the GAIN network. (For now just the GAIN, not GAIN_ext)\n",
    "\n",
    "While I'm not 100% sure what this consists of, I beielive it is as follows:\n",
    " 1. A CNN\n",
    " 2. into a fully connected Layer\n",
    " 3. which outputs a classification loss\n",
    " 4. which is fed into another fully connected layer\n",
    "    (which is stacked together into a ReLu function??? I'm not sure what this step is in the diagram.)\n",
    " 5. which outputs an Attention Map\n",
    " 6. which is used to apply a soft mask onto the original image\n",
    " 7. which is fed into another CNN\n",
    " 8. into another fully connected layer\n",
    " 9. which outputs an Attention Mining Loss\n",
    "10. The Attention Mining Loss is then combined which the Classification loss\n",
    "    (which is then used for backprop???)\n",
    "    \n",
    "    \n",
    "Notes on implementation (from the paper)\n",
    "- It says that in the Classifcation Stream (steps 1-4 above) that that gradients dlowing back should pass through a global average pooling layer, to obtain the weights. I'm not sure how this works exactly, but in the paper it shows a function that the \n",
    "    - weight \n",
    "        - for class c\n",
    "        - of unit k\n",
    "        - in the l-th layer\n",
    "    - should be calulated by taking the GAP (global average pool) of the partial derivative of\n",
    "        - score of class c, DIVIDED BY\n",
    "        - activation (f) of unit k in the l-th layer (which is just the previous weight, no?)\n",
    "- Apparently the above is NOT used for back prop. Rather the weight is the \"importance\" of the actiavtion map (fkl) supporting class c.\n",
    "    - its says that the weights matrix obtained for all layers and units is used as a kernel to apply a 2D convolution for the activation map matrix fl, to integrate all activation maps. Which is then followed by a ReLU function (so i'm assuming this is step 4 above), which should output an attention map.\n",
    "    - Well if I can get that first CNN to ouput that attention map, that would be a great first start.\n",
    "    - Ac = ReLu(conv(fl, wc))\n",
    "    \n",
    "This is a good github repo of an implementation in PyTorch:\n",
    "https://github.com/AustinDoolittle/Pytorch-Gain/blob/master/gain.py\n",
    "- I kindof like keras better, so lets try keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is just copied over from the baseline_cnn_notebook\n",
    "# (with some modification)\n",
    "\n",
    "class ModelParameters:\n",
    "\n",
    "    def __init__(self,\n",
    "                 training_data_path,\n",
    "                 num_classes=28,\n",
    "                 num_epochs=5,\n",
    "                 batch_size=16,\n",
    "                 image_rows=1708,\n",
    "                 image_cols=1708,\n",
    "                 row_scale_factor=4,\n",
    "                 col_scale_factor=4,\n",
    "                 n_channels=3,\n",
    "                 using_imagenet=False,\n",
    "                 shuffle=False):\n",
    "        \n",
    "        self.training_data_path = training_data_path\n",
    "        self.num_classes = num_classes\n",
    "        # what does n_epochs mean? it seems we pass this into the \"epochs\"\n",
    "        # parameter on the \"fit_generator\" method on our keras model\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.row_dimension = np.int(image_rows / row_scale_factor)\n",
    "        self.col_dimension = np.int(image_cols / col_scale_factor)\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        if using_imagenet:\n",
    "            self.row_dimension = 224\n",
    "            self.col_dimension = 224\n",
    "            self.n_channels = 3\n",
    "\n",
    "\n",
    "class ImagePreprocessor:\n",
    "\n",
    "    def __init__(self, modelParameters):\n",
    "        self.image_path = modelParameters.training_data_path\n",
    "        self.n_channels = modelParameters.n_channels\n",
    "        self.row_dimension = modelParameters.row_dimension\n",
    "        self.col_dimension = modelParameters.col_dimension\n",
    "\n",
    "    def preprocess(self, image):\n",
    "        image = cv2.resize(image, (self.row_dimension, self.col_dimension))\n",
    "        # image = np.reshape(image, (image.shape[0], image.shape[1], self.n_channels))\n",
    "        image /= 255\n",
    "        return image\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        image = cv2.imread(self.image_path + image_id + '.jpg')\n",
    "        if self.n_channels == 1:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # Convert Color to Greyscalle (1 channel)\n",
    "            np_image = np.zeros(shape=(image.shape[0], image.shape[1]))\n",
    "        else:\n",
    "            np_image = np.zeros(shape=(image.shape[0], image.shape[1], self.n_channels))\n",
    "        np_image[:,:] = image\n",
    "        return np_image\n",
    "\n",
    "class ImageBatchGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, image_ids, dataframe, model_params, image_processor):\n",
    "        '''\n",
    "        Writing a child implementation of keras.utils.Sequence will help us\n",
    "        manage our batches of data.\n",
    "        Each sequence must implement __len__ and __getitem__\n",
    "        This structure guarantees that the network will only train once on each\n",
    "        sample per epoch which is not the case with generators.\n",
    "        \n",
    "        We can use this class to instantiate training and validation generators\n",
    "        that we can pass into our keras model like:\n",
    "        \n",
    "        training_generator = ImageBatchLoader(...)\n",
    "        validation_generator = ImageBatchLoader(...)\n",
    "        model.set_generators(training_generator, validation_generator)\n",
    "        '''\n",
    "        self.image_ids = image_ids\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "        # Helper classes\n",
    "        self._imageProcessor = image_processor\n",
    "\n",
    "        # Training parameters\n",
    "        self.batch_size = model_params.batch_size\n",
    "        self.dimensions = (model_params.row_dimension, model_params.col_dimension)\n",
    "        self.n_channels = model_params.n_channels\n",
    "        self.shuffle = model_params.shuffle\n",
    "\n",
    "        # Run on_epoch_end in _init_ to init our first image batch\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        '''\n",
    "        Tensorflow will run this method at the end of each epoch\n",
    "        So this is where we will modify our batch.\n",
    "        '''\n",
    "        self.indexes = np.arange(len(self.image_ids))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __len__(self):\n",
    "        # Denotes the number of batchs per epoch\n",
    "        return int(np.floor(len(self.image_ids) / self.batch_size))\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # Get this batches indexes\n",
    "        indexes = self.indexes[index * self.batch_size:(index+1) * self.batch_size]\n",
    "\n",
    "        # Get cooresponding image Ids\n",
    "        batch_image_ids = [self.image_ids[i] for i in indexes]\n",
    "\n",
    "        # Generate one batch of data\n",
    "        X, y = self.__generator(batch_image_ids)\n",
    "        return X, y\n",
    "    \n",
    "    def __generator(self, batch_image_ids):\n",
    "\n",
    "        def get_target_class(imageid):\n",
    "            # .loc will lookup the row where the passed in statement is true\n",
    "            target = self.dataframe.loc[self.dataframe.imageId == imageid]\n",
    "            target = target.lesion.values[0]\n",
    "            return target\n",
    "\n",
    "        X = np.empty((self.batch_size, *self.dimensions, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        for index, imageid in enumerate(batch_image_ids):\n",
    "            image = self._imageProcessor.load_image(imageid)\n",
    "            image = self._imageProcessor.preprocess(image)\n",
    "\n",
    "            X[index] = image\n",
    "            y[index] = get_target_class(imageid)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "class PredictGenerator:\n",
    "\n",
    "    def __init__(self, image_ids, image_processor, image_path):\n",
    "        self.image_processor = image_processor\n",
    "        self.image_processor.image_path = image_path\n",
    "        self.image_ids = image_ids\n",
    "\n",
    "    def predict(self, model):\n",
    "        y = np.empty(shape=(len(self.image_ids)))\n",
    "        for n in range(len(self.image_ids)):\n",
    "            image = self.image_processor.load_image(self.image_ids[n])\n",
    "            image = self.image_processor.preprocess(image)\n",
    "            image = image.reshape((1, *image.shape))\n",
    "            y[n] = model.predict(image)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAIN_Model:\n",
    "    \n",
    "    def __init__(self, model_params):\n",
    "        self.img_rows = model_params.row_dimension\n",
    "        self.img_cols = model_params.col_dimension\n",
    "        self.n_channels = model_params.n_channels\n",
    "        self.input_shape = (self.img_rows, self.img_cols, self.n_channels)\n",
    "        self.num_epochs = model_params.num_epochs\n",
    "        # What metrics should I use for generating the attention map?\n",
    "        # The above linked PyTorch implemntation just uses Accuracy, so I'll stick with that for now.\n",
    "        self.metrics = ['accuracy'] \n",
    "        \n",
    "    def build_model(self):\n",
    "        # Interesting, it looks like in that above linked PyTorch implementation, the classification model\n",
    "        # is just passed in as an argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Reading:\n",
    "- Attention Maps\n",
    "    - https://jacobgil.github.io/deeplearning/class-activation-maps\n",
    "    - https://github.com/jacobgil/keras-cam\n",
    "    - https://github.com/tdeboissiere/VGG16CAM-keras\n",
    "\n",
    "- Getting Gradients\n",
    "    - https://www.programcreek.com/python/example/93762/keras.backend.gradients\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
