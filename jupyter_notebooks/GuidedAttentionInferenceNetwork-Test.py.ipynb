{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this paper: http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_Tell_Me_Where_CVPR_2018_paper.pdf\n",
    "Try to come up with an implementation of the GAIN network. (For now just the GAIN, not GAIN_ext)\n",
    "\n",
    "While I'm not 100% sure what this consists of, I beielive it is as follows:\n",
    " 1. A CNN\n",
    " 2. into a fully connected Layer\n",
    " 3. which outputs a classification loss\n",
    " 4. which is fed into another fully connected layer\n",
    "    (which is stacked together into a ReLu function??? I'm not sure what this step is in the diagram.)\n",
    " 5. which outputs an Attention Map\n",
    " 6. which is used to apply a soft mask onto the original image\n",
    " 7. which is fed into another CNN\n",
    " 8. into another fully connected layer\n",
    " 9. which outputs an Attention Mining Loss\n",
    "10. The Attention Mining Loss is then combined which the Classification loss\n",
    "    (which is then used for backprop???)\n",
    "    \n",
    "    \n",
    "Notes on implementation (from the paper)\n",
    "- It says that in the Classifcation Stream (steps 1-4 above) that that gradients dlowing back should pass through a global average pooling layer, to obtain the weights. I'm not sure how this works exactly, but in the paper it shows a function that the \n",
    "    - weight \n",
    "        - for class c\n",
    "        - of unit k\n",
    "        - in the l-th layer\n",
    "    - should be calulated by taking the GAP (global average pool) of the partial derivative of\n",
    "        - score of class c, DIVIDED BY\n",
    "        - activation (f) of unit k in the l-th layer (which is just the previous weight, no?)\n",
    "- Apparently the above is NOT used for back prop. Rather the weight is the \"importance\" of the actiavtion map (fkl) supporting class c.\n",
    "    - its says that the weights matrix obtained for all layers and units is used as a kernel to apply a 2D convolution for the activation map matrix fl, to integrate all activation maps. Which is then followed by a ReLU function (so i'm assuming this is step 4 above), which should output an attention map.\n",
    "    - Well if I can get that first CNN to ouput that attention map, that would be a great first start.\n",
    "    - Ac = ReLu(conv(fl, wc))\n",
    "    \n",
    "This is a good github repo of an implementation in PyTorch:\n",
    "https://github.com/AustinDoolittle/Pytorch-Gain/blob/master/gain.py\n",
    "- I kindof like keras better, so lets try keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Reading:\n",
    "- Attention Maps\n",
    "    - https://jacobgil.github.io/deeplearning/class-activation-maps\n",
    "    - https://github.com/jacobgil/keras-cam\n",
    "    - https://github.com/tdeboissiere/VGG16CAM-keras\n",
    "\n",
    "- Getting Gradients\n",
    "    - https://www.programcreek.com/python/example/93762/keras.backend.gradients\n",
    "\n",
    "- GAIN Implementation\n",
    "    - https://github.com/AustinDoolittle/Pytorch-Gain/blob/master/gain.py\n",
    "\n",
    "- Branching Models in Keras\n",
    "    - https://www.pyimagesearch.com/2018/06/04/keras-multiple-outputs-and-multiple-losses/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers vars for the notebook\n",
    "already_trained = False\n",
    "train_test_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is just copied over from the baseline_cnn_notebook\n",
    "# (with some modification)\n",
    "\n",
    "class ModelParameters:\n",
    "\n",
    "    def __init__(self,\n",
    "                 training_data_path,\n",
    "                 num_classes=28,\n",
    "                 num_epochs=5,\n",
    "                 batch_size=16,\n",
    "                 image_rows=1708,\n",
    "                 image_cols=1708,\n",
    "                 row_scale_factor=4,\n",
    "                 col_scale_factor=4,\n",
    "                 n_channels=3,\n",
    "                 shuffle=False):\n",
    "        \n",
    "        self.training_data_path = training_data_path\n",
    "        self.num_classes = num_classes\n",
    "        # what does n_epochs mean? it seems we pass this into the \"epochs\"\n",
    "        # parameter on the \"fit_generator\" method on our keras model\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.row_dimension = np.int(image_rows / row_scale_factor)\n",
    "        self.col_dimension = np.int(image_cols / col_scale_factor)\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "\n",
    "class ImagePreprocessor:\n",
    "\n",
    "    def __init__(self, modelParameters):\n",
    "        self.image_path = modelParameters.training_data_path\n",
    "        self.n_channels = modelParameters.n_channels\n",
    "        self.row_dimension = modelParameters.row_dimension\n",
    "        self.col_dimension = modelParameters.col_dimension\n",
    "\n",
    "    def preprocess(self, image):\n",
    "        image = cv2.resize(image, (self.row_dimension, self.col_dimension))\n",
    "        image = np.true_divide(image, 255)\n",
    "        return image\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        return cv2.imread(self.image_path + image_id + '.jpg')\n",
    "\n",
    "class ImageBatchGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self, image_ids, dataframe, model_params, image_processor):\n",
    "        '''\n",
    "        Writing a child implementation of keras.utils.Sequence will help us\n",
    "        manage our batches of data.\n",
    "        Each sequence must implement __len__ and __getitem__\n",
    "        This structure guarantees that the network will only train once on each\n",
    "        sample per epoch which is not the case with generators.\n",
    "        \n",
    "        We can use this class to instantiate training and validation generators\n",
    "        that we can pass into our keras model like:\n",
    "        \n",
    "        training_generator = ImageBatchLoader(...)\n",
    "        validation_generator = ImageBatchLoader(...)\n",
    "        model.set_generators(training_generator, validation_generator)\n",
    "        '''\n",
    "        self.image_ids = image_ids\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "        # Helper classes\n",
    "        self._imageProcessor = image_processor\n",
    "\n",
    "        # Training parameters\n",
    "        self.batch_size = model_params.batch_size\n",
    "        self.dimensions = (model_params.row_dimension, model_params.col_dimension)\n",
    "        self.n_channels = model_params.n_channels\n",
    "        self.shuffle = model_params.shuffle\n",
    "\n",
    "        # Run on_epoch_end in _init_ to init our first image batch\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        '''\n",
    "        Tensorflow will run this method at the end of each epoch\n",
    "        So this is where we will modify our batch.\n",
    "        '''\n",
    "        self.indexes = np.arange(len(self.image_ids))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __len__(self):\n",
    "        # Denotes the number of batchs per epoch\n",
    "        return int(np.floor(len(self.image_ids) / self.batch_size))\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # Get this batches indexes\n",
    "        indexes = self.indexes[index * self.batch_size:(index+1) * self.batch_size]\n",
    "\n",
    "        # Get cooresponding image Ids\n",
    "        batch_image_ids = [self.image_ids[i] for i in indexes]\n",
    "\n",
    "        # Generate one batch of data\n",
    "        X, y = self.__generator(batch_image_ids)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def __generator(self, batch_image_ids):\n",
    "\n",
    "        def get_target_class(imageid):\n",
    "            # .loc will lookup the row where the passed in statement is true\n",
    "            target = self.dataframe.loc[self.dataframe.imageId == imageid]\n",
    "            target = target.lesion.values[0]\n",
    "            return target\n",
    "\n",
    "        X = np.empty((self.batch_size, *self.dimensions, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "        \n",
    "\n",
    "        for index, imageid in enumerate(batch_image_ids):\n",
    "            image = self._imageProcessor.load_image(imageid)\n",
    "            image = self._imageProcessor.preprocess(image)\n",
    "\n",
    "            X[index] = image\n",
    "            y[index] = get_target_class(imageid)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "class PredictGenerator:\n",
    "\n",
    "    def __init__(self, image_ids, image_processor, image_path):\n",
    "        self.image_processor = image_processor\n",
    "        self.image_processor.image_path = image_path\n",
    "        self.image_ids = image_ids\n",
    "\n",
    "    def predict(self, model):\n",
    "        \n",
    "        y = np.empty(shape=(len(self.image_ids)))\n",
    "        \n",
    "        for n in range(len(self.image_ids)):\n",
    "            image = self.image_processor.load_image(self.image_ids[n])\n",
    "            image = self.image_processor.preprocess(image)\n",
    "            image = image.reshape((1, *image.shape))\n",
    "            y[n] = model.predict(image)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "\n",
    "class DotConstant(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, class_output, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.class_output = class_output\n",
    "        super(DotConstant, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[1], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(DotConstant, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "#        return K.dot(x, self.kernel)\n",
    "        return keras.backend.dot(x, self.class_output)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dot, Reshape\n",
    "from keras.layers import GlobalAveragePooling2D, Conv2D, UpSampling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model\n",
    "from keras.applications import DenseNet121\n",
    "\n",
    "class GAIN_Model:\n",
    "    \n",
    "    def __init__(self, model_params):\n",
    "        self.img_rows = model_params.row_dimension\n",
    "        self.img_cols = model_params.col_dimension\n",
    "        self.n_channels = model_params.n_channels\n",
    "        self.input_shape = (self.img_rows, self.img_cols, self.n_channels)\n",
    "        self.num_epochs = model_params.num_epochs\n",
    "        self.metrics = ['accuracy']\n",
    "        \n",
    "    def get_densenet(self):\n",
    "        return DenseNet121(\n",
    "            include_top=False, \n",
    "            weights='imagenet')\n",
    "        \n",
    "    def build_model(self):\n",
    "        # Step 1\n",
    "        # Get DenseNet model\n",
    "        base_model = self.get_densenet()\n",
    "        \n",
    "        # Step 2\n",
    "        # In order to get an attention mask, we need to replace the last layers \n",
    "        # in the DenseNet model with a GAP layer and fully connected softmax layer\n",
    "        last_conv_layer = Conv2D(\n",
    "            self.img_rows, (3, 3), activation=\"relu\", padding=\"same\",\n",
    "            name=\"last_conv_layer\")\n",
    "        last_conv_layer_dense = Dense(512,activation='relu')\n",
    "        \n",
    "        print(\"base model last conv: \" + str(base_model.layers[-3].output.shape))\n",
    "        \n",
    "        x = last_conv_layer(base_model.layers[-3].output)\n",
    "        x = last_conv_layer_dense(x)\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(1024,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
    "        x = Dense(1024,activation='relu')(x) #dense layer 2\n",
    "        x = Dense(512,activation='relu')(x) #dense layer 3\n",
    "        \n",
    "        classification = Dense(1, activation=\"sigmoid\", name=\"classification\")\n",
    "        output = classification(x)\n",
    "        \n",
    "        print(\"new model last conv output: \" + str(last_conv_layer.output.shape))\n",
    "        \n",
    "#         classification_weights = classification.get_weights()[0];\n",
    "#        last_conv_layer_output = last_conv_layer_dense.output\n",
    "#        print(\"last conv dense output: \" + str(last_conv_layer_output.shape))\n",
    "#        last_conv_layer_output = last_conv_layer_output[0, :, :, :]\n",
    "#        print(\"last conv dense output: \" + str(last_conv_layer_output.shape))\n",
    "        \n",
    "#         mat_for_mult = scipy.ndimage.zoom(last_conv_layer_output, (32, 32, 0.5), order=1) # dim: 224 x 224 x 1024\n",
    "#         mat_for_mult = mat_for_mult.reshape(self.img_rows*self.img_cols, 512),\n",
    "\n",
    "#         upsample_conv_output_layer = UpSampling2D((2,2), interpolation='bilinear')\n",
    "#         upsample_conv_output = upsample_conv_output_layer(last_conv_layer_output)\n",
    "        \n",
    "        class_weights = classification.get_weights()[0]\n",
    "        print(class_weights.shape)\n",
    "        class_weights = class_weights[:, 0]\n",
    "        print(class_weights.shape)\n",
    "        \n",
    "        weights_tensor = tf.convert_to_tensor(class_weights, dtype=np.float32)\n",
    "#         weights_tensor_layer = Reshape((1, 512), input_shape=(512,))\n",
    "#         weights_tensor_layer_output = weights_tensor_layer(weights_tensor)\n",
    "        \n",
    "        attention_map = DotConstant(\n",
    "            output_dim=class_weights.shape[0], \n",
    "            class_output=weights_tensor)\n",
    "#        attention_map = Dot(-1, normalize=True)\n",
    "        attn_output = attention_map(last_conv_layer_dense.output)\n",
    "        attention_output = Dense(512, activation='relu')(attn_output)\n",
    "        \n",
    "        self.model = Model(inputs=base_model.input, outputs=[output, attention_output])\n",
    "        \n",
    "    def build_attention_mining_branch(self, inputs):\n",
    "        x = (inputs)\n",
    "    \n",
    "    def generate_heatmap(self, image_id, image_processor):\n",
    "        '''\n",
    "        Next Steps:\n",
    "         - Integrate this into the original model.\n",
    "         - Get a classification loss and an attention mining loss\n",
    "        '''\n",
    "        input_layer = self.model.layers[0].input\n",
    "        output_layer = self.model.layers[-1].output\n",
    "        output_weights = self.model.layers[-1].get_weights()[0]\n",
    "        last_conv_layer = self.model.layers[-7].output\n",
    "\n",
    "        get_outputs = keras.backend.function([input_layer], [last_conv_layer, output_layer])\n",
    "\n",
    "        original_image = image_processor.load_image(image_id)\n",
    "        X = image_processor.preprocess(original_image)\n",
    "        X = X.reshape(1, X.shape[0], X.shape[1], X.shape[2]) # reshape with batch size of 1\n",
    "\n",
    "        [conv_outputs, predictions] = get_outputs([X])\n",
    "        conv_outputs = conv_outputs[0, :, :, :]\n",
    "\n",
    "        mat_for_mult = scipy.ndimage.zoom(conv_outputs, (32, 32, 0.5), order=1) # dim: 224 x 224 x 1024\n",
    "        attention_map = np.dot(\n",
    "            mat_for_mult.reshape(self.img_rows*self.img_cols, 512), \n",
    "            output_weights[:, 0]).reshape(self.img_rows, self.img_cols)\n",
    "\n",
    "        return attention_map\n",
    "    \n",
    "    def compile_model(self):\n",
    "        sgd = SGD(lr=0.01, decay=1e-6, momentum=0.5, nesterov=True)\n",
    "        \n",
    "        losses = {\n",
    "            \"classification\": \"binary_crossentropy\" # TODO: change this to equation 5 fro mthe GAIN paper\n",
    "        }\n",
    "        losses_weights = {\n",
    "            \"classification\": 1.0\n",
    "        }\n",
    "        \n",
    "        self.model.compile(\n",
    "            loss=\"binary_crossentropy\", optimizer=sgd, metrics=self.metrics)\n",
    "    \n",
    "    def set_generators(self, train_generator, validation_generator):\n",
    "        self.training_generator = train_generator\n",
    "        self.validation_generator = validation_generator\n",
    "        \n",
    "    def learn(self):\n",
    "        return self.model.fit_generator(\n",
    "            generator=self.training_generator,\n",
    "            validation_data=self.validation_generator,\n",
    "            epochs=self.num_epochs,\n",
    "            use_multiprocessing=True,\n",
    "            workers=8,\n",
    "            verbose=1)\n",
    "\n",
    "    def score(self):\n",
    "        return self.model.evaluate_generator(\n",
    "            generator=self.validation_generator,\n",
    "            use_multiprocessing=True,\n",
    "            workers=8)\n",
    "\n",
    "    def predict(self, predict_generator):\n",
    "        y = predict_generator.predict(self.model)\n",
    "        return y\n",
    "\n",
    "    def save(self, modeloutputpath):\n",
    "        self.model.save(modeloutputpath)\n",
    "\n",
    "    def load(self, modelinputpath):\n",
    "        self.model = load_model(modelinputpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base model last conv: (?, ?, ?, 1024)\n",
      "new model last conv output: (?, ?, ?, 224)\n",
      "(512, 1)\n",
      "(512,)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Failed to convert object of type <class 'tuple'> to Tensor. Contents: (None, 512). Consider casting elements to a supported type.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m       \u001b[0mstr_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m       \u001b[0mstr_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/util/compat.py\u001b[0m in \u001b[0;36mas_bytes\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     60\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[0;32m---> 61\u001b[0;31m                     (bytes_or_text,))\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected binary or unicode string, got None",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-739fcb0383e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAIN_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-520b5d7e419c>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m             class_output=weights_tensor)\n\u001b[1;32m     73\u001b[0m \u001b[0;31m#        attention_map = Dot(-1, normalize=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_conv_layer_dense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                                          \u001b[0;34m'You can build it manually via: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[0;32m--> 431\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-22cbd67cdbc2>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     13\u001b[0m                                       \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                       \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uniform'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                                       trainable=True)\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDotConstant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Be sure to call this at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         weight = K.variable(initializer(shape),\n\u001b[0m\u001b[1;32m    250\u001b[0m                             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/initializers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         return K.random_uniform(shape, self.minval, self.maxval,\n\u001b[0;32m--> 112\u001b[0;31m                                 dtype=dtype, seed=self.seed)\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, minval, maxval, dtype, seed)\u001b[0m\n\u001b[1;32m   4137\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4138\u001b[0m     return tf.random_uniform(shape, minval=minval, maxval=maxval,\n\u001b[0;32m-> 4139\u001b[0;31m                              dtype=dtype, seed=seed)\n\u001b[0m\u001b[1;32m   4140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/ops/random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, minval, maxval, dtype, seed, name)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0mmaxval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"random_uniform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ShapeTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0mminval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"min\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0mmaxval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/ops/random_ops.py\u001b[0m in \u001b[0;36m_ShapeTensor\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1146\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    227\u001b[0m                                          as_ref=False):\n\u001b[1;32m    228\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    206\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    207\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 208\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    209\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    529\u001b[0m       raise TypeError(\"Failed to convert object of type %s to Tensor. \"\n\u001b[1;32m    530\u001b[0m                       \u001b[0;34m\"Contents: %s. Consider casting elements to a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                       \"supported type.\" % (type(values), values))\n\u001b[0m\u001b[1;32m    532\u001b[0m     \u001b[0mtensor_proto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Failed to convert object of type <class 'tuple'> to Tensor. Contents: (None, 512). Consider casting elements to a supported type."
     ]
    }
   ],
   "source": [
    "gain = GAIN_Model(model_params)\n",
    "gain.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"./lesion-csv.csv\")\n",
    "train_df = pd.read_csv(\"./train.csv\")\n",
    "test_df = pd.read_csv(\"./test.csv\")\n",
    "\n",
    "# All we care about at this point is the id and class\n",
    "train_df = train_df.drop(\n",
    "    [\"Unnamed: 0\", \"Unnamed: 0.1\", \"teethNumbers\", \"description\", \"numberOfCanals\", \"date\", \"sequenceNumber\"], axis=1)\n",
    "\n",
    "test_df = test_df.drop(\n",
    "    [\"Unnamed: 0\", \"Unnamed: 0.1\", \"teethNumbers\", \"description\", \"numberOfCanals\", \"date\", \"sequenceNumber\"], axis=1)\n",
    "\n",
    "partition = {\n",
    "    \"train\": train_df.imageId.values,\n",
    "    \"validation\": test_df.imageId.values,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup params and generators for model\n",
    "\n",
    "train_path = './lesion_images/all_images_processed/'\n",
    "\n",
    "model_params = ModelParameters(\n",
    "    train_path,\n",
    "    num_epochs= 1 if train_test_model else 15,\n",
    "    batch_size=16,\n",
    "    image_rows=224,\n",
    "    image_cols=224,\n",
    "    row_scale_factor=1,\n",
    "    col_scale_factor=1)\n",
    "\n",
    "image_processor = ImagePreprocessor(model_params)\n",
    "\n",
    "training_generator = ImageBatchGenerator(partition['train'], data, model_params, image_processor)\n",
    "validation_generator = ImageBatchGenerator(partition['validation'], data, model_params, image_processor)\n",
    "predict_generator = PredictGenerator(partition['validation'], image_processor, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not already_trained:\n",
    "    print(\"Training Model\")\n",
    "    gain = GAIN_Model(model_params)\n",
    "    gain.build_model()\n",
    "    gain.compile_model()\n",
    "    gain.set_generators(training_generator, validation_generator)\n",
    "    history = gain.learn()\n",
    "    \n",
    "#     filename = \"test_model.h5\" if train_test_model else \"attention_mask_model.h5\"\n",
    "#     gain.save(filename)\n",
    "    model = gain.model\n",
    "#     already_trained = True\n",
    "else:\n",
    "    print(\"Loading Model\")\n",
    "    filename = \"test_model.h5\" if train_test_model else \"attention_mask_model.h5\"\n",
    "    model = load_model(\"./\"+ filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "heatmap = generate_heatmap(model, \"0001-0\", image_processor)\n",
    "original_image = image_processor.load_image(\"0001-0\")\n",
    "\n",
    "ax.imshow(original_image)\n",
    "ax.imshow(heatmap, cmap='jet', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
